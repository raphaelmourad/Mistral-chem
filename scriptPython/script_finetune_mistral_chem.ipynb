{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Raphael Mourad\n",
    "### Associate Professor\n",
    "### University Paul Sabatier / INRAE MIAT Lab Toulouse\n",
    "### 22/01/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to fine tune mixtral-dna labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.18 | packaged by conda-forge | (default, Dec 23 2023, 17:21:28) \n",
      "[GCC 12.3.0]\n",
      "1.23.5\n",
      "2.2.0\n",
      "4.37.2\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkProcess-9:\n",
      "Process ForkProcess-11:\n",
      "Process ForkProcess-6:\n",
      "Process ForkProcess-12:\n",
      "Process ForkProcess-7:\n",
      "Process ForkProcess-15:\n",
      "Process ForkProcess-10:\n",
      "Process ForkProcess-13:\n",
      "Process ForkProcess-8:\n",
      "Process ForkProcess-3:\n",
      "Process ForkProcess-5:\n",
      "Process ForkProcess-16:\n",
      "Process ForkProcess-1:\n",
      "Process ForkProcess-14:\n",
      "Process ForkProcess-4:\n",
      "Process ForkProcess-2:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/queues.py\", line 97, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "### LOAD PYTHON MODULES\n",
    "# Load basic modules\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from os import path\n",
    "import gc\n",
    "\n",
    "# Load data and machine learning modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randrange\n",
    "from progressbar import ProgressBar\n",
    "\n",
    "import torch\n",
    "import triton\n",
    "import transformers\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "from transformers import AutoTokenizer, AutoModel, EarlyStoppingCallback, set_seed, BitsAndBytesConfig\n",
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Print numpy version for compatibility\n",
    "print(sys.version)\n",
    "print(np.__version__)\n",
    "print(triton.__version__)\n",
    "print(transformers.__version__)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mourad/miniconda3/envs/mistral_dna\n"
     ]
    }
   ],
   "source": [
    "### CHECK ENV\n",
    "print(sys.prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mourad/Téléchargements/Mistral-chem\n"
     ]
    }
   ],
   "source": [
    "### SET DIRECTORY\n",
    "os.chdir(\"/home/mourad/Téléchargements/Mistral-chem/\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPECIFY PARAMETERS\n",
    "model_name=\"RaphaelMourad/mixtral-chem-v0.1\" \n",
    "lora=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:kipoi_utils.external.related.mixins:Unrecognized fields for DataLoaderDescription: {'postprocessing'}. Available fields are {'dependencies', 'info', 'args', 'path', 'defined_as', 'output_schema', 'writers', 'type'}\n",
      "WARNING:kipoi_utils.external.related.mixins:Unrecognized fields for DataLoaderDescription: {'postprocessing'}. Available fields are {'dependencies', 'info', 'args', 'path', 'defined_as', 'output_schema', 'writers', 'type'}\n",
      "WARNING:kipoi_utils.external.related.mixins:Unrecognized fields for DataLoaderDescription: {'postprocessing'}. Available fields are {'dependencies', 'info', 'args', 'path', 'defined_as', 'output_schema', 'writers', 'type'}\n"
     ]
    }
   ],
   "source": [
    "### LOAD FUNCTIONS MODULE\n",
    "sys.path.append(\"scriptPython/\")\n",
    "from functions_chem import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataArguments(data_path=None, kmer=-1)\n",
      "ModelArguments(model_name_or_path='facebook/opt-125m', use_lora=True, lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target_modules='query,value')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "functions_chem.TrainingArguments"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRAINING PARAMETERS\n",
    "data_args=DataArguments()\n",
    "print(data_args)\n",
    "\n",
    "model_args=ModelArguments()\n",
    "model_args.use_lora=lora\n",
    "print(model_args)\n",
    "\n",
    "training_args=TrainingArguments\n",
    "training_args.deepspeed_plugin=None\n",
    "training_args.run_name=model_name\n",
    "training_args.model_max_length=1024 # max sequence length (can be increased)\n",
    "training_args.gradient_accumulation_steps=1\n",
    "training_args.learning_rate=5e-5\n",
    "training_args.num_train_epochs=10\n",
    "training_args.fp16=True \n",
    "training_args.save_steps=5000\n",
    "training_args.evaluation_strategy=\"epoch\"\n",
    "training_args.warmup_steps=50\n",
    "training_args.load_best_model_at_end=True\n",
    "training_args.logging_steps=100000\n",
    "training_args.find_unused_parameters=False\n",
    "\n",
    "# Other arguments to add since it was bugging\n",
    "bs=8\n",
    "training_args.device=torch.device('cuda:0')\n",
    "training_args.report_to=[\"tensorboard\"]\n",
    "training_args.world_size=1\n",
    "training_args.train_batch_size=bs\n",
    "training_args.eval_batch_size=bs\n",
    "training_args.test_batch_size=bs\n",
    "training_args.batch_size=bs\n",
    "training_args.num_training_steps=100\n",
    "training_args.n_gpu=1\n",
    "training_args.distributed_state=None\n",
    "training_args.local_rank=-1 # -1\n",
    "training_args.metric_for_best_model=\"eval_loss\"\n",
    "\n",
    "training_args.fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False, 'xla_device': 'cpu'}\n",
    "training_args.lr_scheduler_kwargs={}\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BitsAndBytesConfig {\n",
       "  \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
       "  \"bnb_4bit_quant_type\": \"fp4\",\n",
       "  \"bnb_4bit_use_double_quant\": true,\n",
       "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "  \"llm_int8_has_fp16_weight\": false,\n",
       "  \"llm_int8_skip_modules\": null,\n",
       "  \"llm_int8_threshold\": 6.0,\n",
       "  \"load_in_4bit\": true,\n",
       "  \"load_in_8bit\": false,\n",
       "  \"quant_method\": \"bitsandbytes\"\n",
       "}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONFIG QUANTIZATION\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "bnb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG ACCELERATE\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG LORA\n",
    "peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431\n",
      "1711\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCCCCCCC/C=C\\CCCCCCCC(N)=O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCCCCCOC(=O)c1ccccc1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O=C(c1ccc(Cl)cc1)c1ccc(Cl)cc1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COc1cc(Cl)c(OC)cc1N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N[C@H](Cc1c[nH]c2ccccc12)C(=O)O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137</th>\n",
       "      <td>CCCCCCCC(=O)OC</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2138</th>\n",
       "      <td>CN1Cc2c(N)cccc2C(c2ccccc2)C1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2139</th>\n",
       "      <td>CCC(=O)O[C@H]1CC[C@H]2[C@@H]3CCc4cc(O)ccc4[C@H...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2140</th>\n",
       "      <td>Cc1ncsc1CCCl</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2141</th>\n",
       "      <td>CCCCCCC/C=C/C=O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2142 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0                            CCCCCCCC/C=C\\CCCCCCCC(N)=O      0\n",
       "1                                  CCCCCCOC(=O)c1ccccc1      0\n",
       "2                         O=C(c1ccc(Cl)cc1)c1ccc(Cl)cc1      0\n",
       "3                                   COc1cc(Cl)c(OC)cc1N      0\n",
       "4                       N[C@H](Cc1c[nH]c2ccccc12)C(=O)O      0\n",
       "...                                                 ...    ...\n",
       "2137                                     CCCCCCCC(=O)OC      0\n",
       "2138                       CN1Cc2c(N)cccc2C(c2ccccc2)C1      0\n",
       "2139  CCC(=O)O[C@H]1CC[C@H]2[C@@H]3CCc4cc(O)ccc4[C@H...      1\n",
       "2140                                       Cc1ncsc1CCCl      0\n",
       "2141                                    CCCCCCC/C=C/C=O      0\n",
       "\n",
       "[2142 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "81\n",
      "94\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "#labeled_data=pd.read_csv(\"data/chemistry/finetune/antibiotic_ecoli_growth.csv\",sep=',')\n",
    "labeled_data=pd.read_csv(\"data/chemistry/finetune/tox21_balanced_revised_no_id.csv\",sep=',')\n",
    "print(np.sum(labeled_data[\"label\"]))\n",
    "print(np.sum(labeled_data[\"label\"]==0))\n",
    "display(labeled_data)\n",
    "\n",
    "traintmp_df, test_df = train_test_split(labeled_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Make balanced data\n",
    "traintmp_df_pos=traintmp_df[traintmp_df.label==1]\n",
    "traintmp_df_neg=traintmp_df[traintmp_df.label==0]\n",
    "traintmp_df_neg=traintmp_df_neg.sample(len(traintmp_df_pos))\n",
    "traintmp_df=pd.concat((traintmp_df_pos,traintmp_df_neg))\n",
    "traintmp_df\n",
    "\n",
    "train_df, valid_df = train_test_split(traintmp_df, test_size=0.2, random_state=42)\n",
    "print(np.sum(train_df[\"label\"]))\n",
    "print(np.sum(valid_df[\"label\"]))\n",
    "print(np.sum(test_df[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.json from cache at /home/mourad/.cache/huggingface/hub/models--RaphaelMourad--mixtral-chem-v0.1/snapshots/8488e266c3852ac476bc55be77db219927cbf86a/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/mourad/.cache/huggingface/hub/models--RaphaelMourad--mixtral-chem-v0.1/snapshots/8488e266c3852ac476bc55be77db219927cbf86a/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/mourad/.cache/huggingface/hub/models--RaphaelMourad--mixtral-chem-v0.1/snapshots/8488e266c3852ac476bc55be77db219927cbf86a/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='RaphaelMourad/mixtral-chem-v0.1', vocab_size=1024, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# LOAD TOKENIZER\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.padding_side  = 'left'\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# MAKE DATA FOR GPT NEO\n",
    "train_encodings = tokenizer(train_df[\"text\"].values.tolist(), return_tensors=\"pt\",padding=True,truncation=True)\n",
    "val_encodings = tokenizer(valid_df[\"text\"].values.tolist(), return_tensors=\"pt\",padding=True,truncation=True)\n",
    "test_encodings = tokenizer(test_df[\"text\"].values.tolist(), return_tensors=\"pt\",padding=True,truncation=True)\n",
    "\n",
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = NewsDataset(train_encodings, train_df[\"label\"].values.tolist())\n",
    "val_dataset = NewsDataset(val_encodings, valid_df[\"label\"].values.tolist())\n",
    "test_dataset = NewsDataset(test_encodings, test_df[\"label\"].values.tolist())\n",
    "\n",
    "# define datasets and data collator      \n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/mourad/.cache/huggingface/hub/models--RaphaelMourad--mixtral-chem-v0.1/snapshots/8488e266c3852ac476bc55be77db219927cbf86a/config.json\n",
      "Model config MixtralConfig {\n",
      "  \"_name_or_path\": \"RaphaelMourad/mixtral-chem-v0.1\",\n",
      "  \"architectures\": [\n",
      "    \"MixtralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 768,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"mixtral\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_experts_per_tok\": 1,\n",
      "  \"num_hidden_layers\": 8,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_local_experts\": 64,\n",
      "  \"output_router_logits\": false,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"router_aux_loss_coef\": 0.02,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 4096\n",
      "}\n",
      "\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n",
      "loading weights file model.safetensors from cache at /home/mourad/.cache/huggingface/hub/models--RaphaelMourad--mixtral-chem-v0.1/snapshots/8488e266c3852ac476bc55be77db219927cbf86a/model.safetensors\n",
      "Instantiating MixtralForSequenceClassification model under default dtype torch.float16.\n",
      "Detected 4-bit loading: activating 4-bit loading for this model\n",
      "Some weights of the model checkpoint at RaphaelMourad/mixtral-chem-v0.1 were not used when initializing MixtralForSequenceClassification: ['lm_head.weight']\n",
      "- This IS expected if you are initializing MixtralForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MixtralForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MixtralForSequenceClassification were not initialized from the model checkpoint at RaphaelMourad/mixtral-chem-v0.1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using auto half precision backend\n",
      "Currently training with a batch size of: 8\n",
      "***** Running training *****\n",
      "  Num examples = 539\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Training with DataParallel so batch size has been adjusted to: 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 680\n",
      "  Number of trainable parameters = 787,968\n",
      "/tmp/ipykernel_68308/1806923087.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "The following columns in the training set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='680' max='680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [680/680 12:20, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.672723</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.583450</td>\n",
       "      <td>0.170138</td>\n",
       "      <td>0.583739</td>\n",
       "      <td>0.586420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.635588</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.622145</td>\n",
       "      <td>0.248800</td>\n",
       "      <td>0.622293</td>\n",
       "      <td>0.626543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.607215</td>\n",
       "      <td>0.681481</td>\n",
       "      <td>0.667164</td>\n",
       "      <td>0.334405</td>\n",
       "      <td>0.667740</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.582703</td>\n",
       "      <td>0.696296</td>\n",
       "      <td>0.684597</td>\n",
       "      <td>0.369274</td>\n",
       "      <td>0.684091</td>\n",
       "      <td>0.685185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.571612</td>\n",
       "      <td>0.696296</td>\n",
       "      <td>0.686384</td>\n",
       "      <td>0.373482</td>\n",
       "      <td>0.685223</td>\n",
       "      <td>0.688272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.562362</td>\n",
       "      <td>0.674074</td>\n",
       "      <td>0.662500</td>\n",
       "      <td>0.325306</td>\n",
       "      <td>0.661731</td>\n",
       "      <td>0.663580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.555253</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.679589</td>\n",
       "      <td>0.360427</td>\n",
       "      <td>0.678348</td>\n",
       "      <td>0.682099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.562834</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.657573</td>\n",
       "      <td>0.317026</td>\n",
       "      <td>0.656557</td>\n",
       "      <td>0.660494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.559357</td>\n",
       "      <td>0.651852</td>\n",
       "      <td>0.640490</td>\n",
       "      <td>0.281642</td>\n",
       "      <td>0.639676</td>\n",
       "      <td>0.641975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.557784</td>\n",
       "      <td>0.651852</td>\n",
       "      <td>0.640490</td>\n",
       "      <td>0.281642</td>\n",
       "      <td>0.639676</td>\n",
       "      <td>0.641975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 135\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/tmp/ipykernel_68308/1806923087.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 135\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/tmp/ipykernel_68308/1806923087.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 135\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/tmp/ipykernel_68308/1806923087.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 135\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/tmp/ipykernel_68308/1806923087.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 135\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/tmp/ipykernel_68308/1806923087.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 135\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/tmp/ipykernel_68308/1806923087.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 135\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/tmp/ipykernel_68308/1806923087.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 135\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68308/1806923087.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 135\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/tmp/ipykernel_68308/1806923087.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 135\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=680, training_loss=0.6015116523293887, metrics={'train_runtime': 741.9391, 'train_samples_per_second': 7.265, 'train_steps_per_second': 0.917, 'total_flos': 7846409376368640.0, 'train_loss': 0.6015116523293887, 'epoch': 10.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### CREATE AND TRAIN MODEL\n",
    "num_labels=2\n",
    "\n",
    "model=transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    output_hidden_states=False,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda:0',\n",
    ")\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model = accelerator.prepare_model(model)\n",
    "\n",
    "# Setup trainer\n",
    "trainer = transformers.Trainer(model=model,\n",
    "                               args=training_args,\n",
    "                               compute_metrics=compute_metrics,\n",
    "                               train_dataset=train_dataset, \n",
    "                               eval_dataset=val_dataset,\n",
    "                               data_collator=data_collator,\n",
    "                              callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "                              )        \n",
    "trainer.local_rank=training_args.local_rank\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68308/1806923087.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100% |#####################################################################################################|\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5101302000635122"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PREDICT ON TEST DATA\n",
    "bst=32\n",
    "pred_test=[]\n",
    "idx=range(0,len(test_dataset),bst)\n",
    "pbar = ProgressBar()\n",
    "for i in pbar(idx):\n",
    "    batch=test_dataset[i:(i+bst)]\n",
    "    output = model(batch['input_ids'].cuda())\n",
    "    pred_test.append(output[0][:,1].detach())\n",
    "    del output, batch\n",
    "    gc.collect()\n",
    "\n",
    "y_pred=torch.concatenate(pred_test,dim=0)\n",
    "y_pred=torch.sigmoid(y_pred)\n",
    "y_pred=y_pred.cpu().detach().numpy()\n",
    "\n",
    "roc_auc_score(test_dataset.labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral_dna",
   "language": "python",
   "name": "mistral_dna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
