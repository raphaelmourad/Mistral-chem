{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Raphael Mourad\n",
    "### Associate Professor\n",
    "### University Paul Sabatier / INRAE MIAT Lab Toulouse\n",
    "### 22/01/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to fine tune mixtral-dna labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.18 | packaged by conda-forge | (default, Dec 23 2023, 17:21:28) \n",
      "[GCC 12.3.0]\n",
      "1.23.5\n",
      "2.2.0\n",
      "4.37.2\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "### LOAD PYTHON MODULES\n",
    "# Load basic modules\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from os import path\n",
    "import gc\n",
    "\n",
    "# Load data and machine learning modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randrange\n",
    "from progressbar import ProgressBar\n",
    "\n",
    "import torch\n",
    "import triton\n",
    "import transformers\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "from transformers import AutoTokenizer, AutoModel, EarlyStoppingCallback, set_seed, BitsAndBytesConfig\n",
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Print numpy version for compatibility\n",
    "print(sys.version)\n",
    "print(np.__version__)\n",
    "print(triton.__version__)\n",
    "print(transformers.__version__)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mourad/miniconda3/envs/mistral_dna\n"
     ]
    }
   ],
   "source": [
    "### CHECK ENV\n",
    "print(sys.prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mourad/Téléchargements/Mistral-chem\n"
     ]
    }
   ],
   "source": [
    "### SET DIRECTORY\n",
    "os.chdir(\"/home/mourad/Téléchargements/Mistral-chem/\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPECIFY PARAMETERS\n",
    "model_name=\"RaphaelMourad/mixtral-chem-v0.4\" \n",
    "lora=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD FUNCTIONS MODULE\n",
    "sys.path.append(\"scriptPython/\")\n",
    "from functions_chem import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataArguments(data_path=None, kmer=-1)\n",
      "ModelArguments(model_name_or_path='facebook/opt-125m', use_lora=True, lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target_modules='query,value')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "functions_chem.TrainingArguments"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRAINING PARAMETERS\n",
    "data_args=DataArguments()\n",
    "print(data_args)\n",
    "\n",
    "model_args=ModelArguments()\n",
    "model_args.use_lora=lora\n",
    "print(model_args)\n",
    "\n",
    "training_args=TrainingArguments\n",
    "training_args.deepspeed_plugin=None\n",
    "training_args.run_name=model_name\n",
    "training_args.model_max_length=1024 # max sequence length (can be increased)\n",
    "training_args.gradient_accumulation_steps=1\n",
    "training_args.learning_rate=2e-5\n",
    "training_args.num_train_epochs=10\n",
    "training_args.fp16=True \n",
    "training_args.save_steps=5000\n",
    "training_args.evaluation_strategy=\"epoch\"\n",
    "training_args.warmup_steps=50\n",
    "training_args.load_best_model_at_end=True\n",
    "training_args.logging_steps=100000\n",
    "training_args.find_unused_parameters=False\n",
    "\n",
    "# Other arguments to add since it was bugging\n",
    "bs=32\n",
    "training_args.device=torch.device('cuda:0')\n",
    "training_args.report_to=[\"tensorboard\"]\n",
    "training_args.world_size=1\n",
    "training_args.train_batch_size=bs\n",
    "training_args.eval_batch_size=bs\n",
    "training_args.test_batch_size=bs\n",
    "training_args.batch_size=bs\n",
    "training_args.num_training_steps=100\n",
    "training_args.n_gpu=1\n",
    "training_args.distributed_state=None\n",
    "training_args.local_rank=-1 # -1\n",
    "training_args.metric_for_best_model=\"eval_loss\"\n",
    "\n",
    "training_args.fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False, 'xla_device': 'cpu'}\n",
    "training_args.lr_scheduler_kwargs={}\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BitsAndBytesConfig {\n",
       "  \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
       "  \"bnb_4bit_quant_type\": \"fp4\",\n",
       "  \"bnb_4bit_use_double_quant\": true,\n",
       "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "  \"llm_int8_has_fp16_weight\": false,\n",
       "  \"llm_int8_skip_modules\": null,\n",
       "  \"llm_int8_threshold\": 6.0,\n",
       "  \"load_in_4bit\": true,\n",
       "  \"load_in_8bit\": false,\n",
       "  \"quant_method\": \"bitsandbytes\"\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONFIG QUANTIZATION\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "bnb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG ACCELERATE\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG LORA\n",
    "peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1911\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pIC50</th>\n",
       "      <th>mol</th>\n",
       "      <th>num_atoms</th>\n",
       "      <th>logP</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O=S(=O)(Nc1cccc(-c2cnc3ccccc3n2)c1)c1cccs1</td>\n",
       "      <td>4.26</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7f59df45bc30&gt;</td>\n",
       "      <td>25</td>\n",
       "      <td>4.15910</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O=c1cc(-c2nc(-c3ccc(-c4cn(CCP(=O)(O)O)nn4)cc3)...</td>\n",
       "      <td>4.34</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7f59a320c9e0&gt;</td>\n",
       "      <td>36</td>\n",
       "      <td>3.67430</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NC(=O)c1ccc2c(c1)nc(C1CCC(O)CC1)n2CCCO</td>\n",
       "      <td>4.53</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7f59a320cac0&gt;</td>\n",
       "      <td>23</td>\n",
       "      <td>1.53610</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCCCn1c(C2CCNCC2)nc2cc(C(N)=O)ccc21</td>\n",
       "      <td>4.56</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7f59a320cba0&gt;</td>\n",
       "      <td>22</td>\n",
       "      <td>0.95100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNC(=S)Nc1cccc(-c2cnc3ccccc3n2)c1</td>\n",
       "      <td>4.59</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7f59a320c7b0&gt;</td>\n",
       "      <td>21</td>\n",
       "      <td>3.21300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16082</th>\n",
       "      <td>S=C(NN=C(c1ccccn1)c1ccccn1)Nc1ccccc1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7f59a314ed50&gt;</td>\n",
       "      <td>24</td>\n",
       "      <td>3.21560</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16083</th>\n",
       "      <td>S=C=NCCCCCCCCCCc1ccccc1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7f59a314edc0&gt;</td>\n",
       "      <td>19</td>\n",
       "      <td>5.45270</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16084</th>\n",
       "      <td>S=C=NCCCCCCCCc1ccccc1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7f59a314ee30&gt;</td>\n",
       "      <td>17</td>\n",
       "      <td>4.67250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16085</th>\n",
       "      <td>S=c1[nH]nc(Cn2ccc3ccccc32)n1-c1ccccc1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7f59a314eea0&gt;</td>\n",
       "      <td>22</td>\n",
       "      <td>3.93289</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16086</th>\n",
       "      <td>N=[N+]=NC=Cc1ccc([N+](=O)[O-])o1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7f59a314ef10&gt;</td>\n",
       "      <td>13</td>\n",
       "      <td>1.70887</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16087 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  pIC50  \\\n",
       "0             O=S(=O)(Nc1cccc(-c2cnc3ccccc3n2)c1)c1cccs1   4.26   \n",
       "1      O=c1cc(-c2nc(-c3ccc(-c4cn(CCP(=O)(O)O)nn4)cc3)...   4.34   \n",
       "2                 NC(=O)c1ccc2c(c1)nc(C1CCC(O)CC1)n2CCCO   4.53   \n",
       "3                    NCCCn1c(C2CCNCC2)nc2cc(C(N)=O)ccc21   4.56   \n",
       "4                      CNC(=S)Nc1cccc(-c2cnc3ccccc3n2)c1   4.59   \n",
       "...                                                  ...    ...   \n",
       "16082               S=C(NN=C(c1ccccn1)c1ccccn1)Nc1ccccc1   0.00   \n",
       "16083                            S=C=NCCCCCCCCCCc1ccccc1   0.00   \n",
       "16084                              S=C=NCCCCCCCCc1ccccc1   0.00   \n",
       "16085              S=c1[nH]nc(Cn2ccc3ccccc32)n1-c1ccccc1   0.00   \n",
       "16086                   N=[N+]=NC=Cc1ccc([N+](=O)[O-])o1   0.02   \n",
       "\n",
       "                                                    mol  num_atoms     logP  \\\n",
       "0      <rdkit.Chem.rdchem.Mol object at 0x7f59df45bc30>         25  4.15910   \n",
       "1      <rdkit.Chem.rdchem.Mol object at 0x7f59a320c9e0>         36  3.67430   \n",
       "2      <rdkit.Chem.rdchem.Mol object at 0x7f59a320cac0>         23  1.53610   \n",
       "3      <rdkit.Chem.rdchem.Mol object at 0x7f59a320cba0>         22  0.95100   \n",
       "4      <rdkit.Chem.rdchem.Mol object at 0x7f59a320c7b0>         21  3.21300   \n",
       "...                                                 ...        ...      ...   \n",
       "16082  <rdkit.Chem.rdchem.Mol object at 0x7f59a314ed50>         24  3.21560   \n",
       "16083  <rdkit.Chem.rdchem.Mol object at 0x7f59a314edc0>         19  5.45270   \n",
       "16084  <rdkit.Chem.rdchem.Mol object at 0x7f59a314ee30>         17  4.67250   \n",
       "16085  <rdkit.Chem.rdchem.Mol object at 0x7f59a314eea0>         22  3.93289   \n",
       "16086  <rdkit.Chem.rdchem.Mol object at 0x7f59a314ef10>         13  1.70887   \n",
       "\n",
       "       label  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "...      ...  \n",
       "16082      0  \n",
       "16083      0  \n",
       "16084      0  \n",
       "16085      0  \n",
       "16086      0  \n",
       "\n",
       "[16087 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1911\n",
      "14176\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pIC50</th>\n",
       "      <th>mol</th>\n",
       "      <th>num_atoms</th>\n",
       "      <th>logP</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O=S(=O)(Nc1cccc(-c2cnc3ccccc3n2)c1)c1cccs1</td>\n",
       "      <td>4.26</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7f59df45bc30&gt;</td>\n",
       "      <td>25</td>\n",
       "      <td>4.15910</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O=c1cc(-c2nc(-c3ccc(-c4cn(CCP(=O)(O)O)nn4)cc3)...</td>\n",
       "      <td>4.34</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7f59a320c9e0&gt;</td>\n",
       "      <td>36</td>\n",
       "      <td>3.67430</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NC(=O)c1ccc2c(c1)nc(C1CCC(O)CC1)n2CCCO</td>\n",
       "      <td>4.53</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7f59a320cac0&gt;</td>\n",
       "      <td>23</td>\n",
       "      <td>1.53610</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCCCn1c(C2CCNCC2)nc2cc(C(N)=O)ccc21</td>\n",
       "      <td>4.56</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7f59a320cba0&gt;</td>\n",
       "      <td>22</td>\n",
       "      <td>0.95100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNC(=S)Nc1cccc(-c2cnc3ccccc3n2)c1</td>\n",
       "      <td>4.59</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7f59a320c7b0&gt;</td>\n",
       "      <td>21</td>\n",
       "      <td>3.21300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16082</th>\n",
       "      <td>S=C(NN=C(c1ccccn1)c1ccccn1)Nc1ccccc1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7f59a314ed50&gt;</td>\n",
       "      <td>24</td>\n",
       "      <td>3.21560</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16083</th>\n",
       "      <td>S=C=NCCCCCCCCCCc1ccccc1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7f59a314edc0&gt;</td>\n",
       "      <td>19</td>\n",
       "      <td>5.45270</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16084</th>\n",
       "      <td>S=C=NCCCCCCCCc1ccccc1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7f59a314ee30&gt;</td>\n",
       "      <td>17</td>\n",
       "      <td>4.67250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16085</th>\n",
       "      <td>S=c1[nH]nc(Cn2ccc3ccccc32)n1-c1ccccc1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7f59a314eea0&gt;</td>\n",
       "      <td>22</td>\n",
       "      <td>3.93289</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16086</th>\n",
       "      <td>N=[N+]=NC=Cc1ccc([N+](=O)[O-])o1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>&lt;rdkit.Chem.rdchem.Mol object at 0x7f59a314ef10&gt;</td>\n",
       "      <td>13</td>\n",
       "      <td>1.70887</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16087 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  pIC50  \\\n",
       "0             O=S(=O)(Nc1cccc(-c2cnc3ccccc3n2)c1)c1cccs1   4.26   \n",
       "1      O=c1cc(-c2nc(-c3ccc(-c4cn(CCP(=O)(O)O)nn4)cc3)...   4.34   \n",
       "2                 NC(=O)c1ccc2c(c1)nc(C1CCC(O)CC1)n2CCCO   4.53   \n",
       "3                    NCCCn1c(C2CCNCC2)nc2cc(C(N)=O)ccc21   4.56   \n",
       "4                      CNC(=S)Nc1cccc(-c2cnc3ccccc3n2)c1   4.59   \n",
       "...                                                  ...    ...   \n",
       "16082               S=C(NN=C(c1ccccn1)c1ccccn1)Nc1ccccc1   0.00   \n",
       "16083                            S=C=NCCCCCCCCCCc1ccccc1   0.00   \n",
       "16084                              S=C=NCCCCCCCCc1ccccc1   0.00   \n",
       "16085              S=c1[nH]nc(Cn2ccc3ccccc32)n1-c1ccccc1   0.00   \n",
       "16086                   N=[N+]=NC=Cc1ccc([N+](=O)[O-])o1   0.02   \n",
       "\n",
       "                                                    mol  num_atoms     logP  \\\n",
       "0      <rdkit.Chem.rdchem.Mol object at 0x7f59df45bc30>         25  4.15910   \n",
       "1      <rdkit.Chem.rdchem.Mol object at 0x7f59a320c9e0>         36  3.67430   \n",
       "2      <rdkit.Chem.rdchem.Mol object at 0x7f59a320cac0>         23  1.53610   \n",
       "3      <rdkit.Chem.rdchem.Mol object at 0x7f59a320cba0>         22  0.95100   \n",
       "4      <rdkit.Chem.rdchem.Mol object at 0x7f59a320c7b0>         21  3.21300   \n",
       "...                                                 ...        ...      ...   \n",
       "16082  <rdkit.Chem.rdchem.Mol object at 0x7f59a314ed50>         24  3.21560   \n",
       "16083  <rdkit.Chem.rdchem.Mol object at 0x7f59a314edc0>         19  5.45270   \n",
       "16084  <rdkit.Chem.rdchem.Mol object at 0x7f59a314ee30>         17  4.67250   \n",
       "16085  <rdkit.Chem.rdchem.Mol object at 0x7f59a314eea0>         22  3.93289   \n",
       "16086  <rdkit.Chem.rdchem.Mol object at 0x7f59a314ef10>         13  1.70887   \n",
       "\n",
       "       label  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "...      ...  \n",
       "16082      0  \n",
       "16083      0  \n",
       "16084      0  \n",
       "16085      0  \n",
       "16086      0  \n",
       "\n",
       "[16087 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1227\n",
      "319\n",
      "365\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "#labeled_data=pd.read_csv(\"data/chemistry/finetune/antibiotic_ecoli_growth.csv\",sep=',')\n",
    "# \"pIC50\" - is a measurement used in pharmacology and drug discovery \n",
    "# to assess the potency of a compound in inhibiting a specific biological target or enzyme\n",
    "chem_data=pd.read_csv(\"data/chemistry/finetune/SMILES_Big_Data_Set.csv\",sep=',')\n",
    "chem_data[\"label\"]=chem_data[\"pIC50\"]>1\n",
    "chem_data[\"label\"]=chem_data[\"label\"].astype(\"int\")\n",
    "print(np.sum(chem_data[\"label\"]))\n",
    "display(chem_data)\n",
    "\n",
    "print(np.sum(chem_data[\"label\"]))\n",
    "print(np.sum(chem_data[\"label\"]==0))\n",
    "display(chem_data)\n",
    "\n",
    "traintmp_df, test_df = train_test_split(chem_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Make balanced data\n",
    "#traintmp_df_pos=traintmp_df[traintmp_df.label==1]\n",
    "#traintmp_df_neg=traintmp_df[traintmp_df.label==0]\n",
    "#traintmp_df_neg=traintmp_df_neg.sample(len(traintmp_df_pos))\n",
    "#traintmp_df=pd.concat((traintmp_df_pos,traintmp_df_neg))\n",
    "#traintmp_df\n",
    "\n",
    "train_df, valid_df = train_test_split(traintmp_df, test_size=0.2, random_state=42)\n",
    "print(np.sum(train_df[\"label\"]))\n",
    "print(np.sum(valid_df[\"label\"]))\n",
    "print(np.sum(test_df[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████████████████████████| 1.14k/1.14k [00:00<00:00, 123kB/s]\n",
      "tokenizer.json: 100%|█████████████████████████████████████| 38.2k/38.2k [00:00<00:00, 486kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='RaphaelMourad/mixtral-chem-v0.3', vocab_size=1024, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# LOAD TOKENIZER\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.padding_side  = 'left'\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# MAKE DATA \n",
    "train_encodings = tokenizer(train_df[\"text\"].values.tolist(), return_tensors=\"pt\",padding=True,truncation=True)\n",
    "val_encodings = tokenizer(valid_df[\"text\"].values.tolist(), return_tensors=\"pt\",padding=True,truncation=True)\n",
    "test_encodings = tokenizer(test_df[\"text\"].values.tolist(), return_tensors=\"pt\",padding=True,truncation=True)\n",
    "\n",
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = NewsDataset(train_encodings, train_df[\"label\"].values.tolist())\n",
    "val_dataset = NewsDataset(val_encodings, valid_df[\"label\"].values.tolist())\n",
    "test_dataset = NewsDataset(test_encodings, test_df[\"label\"].values.tolist())\n",
    "\n",
    "# define datasets and data collator      \n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MixtralForSequenceClassification were not initialized from the model checkpoint at RaphaelMourad/mixtral-chem-v0.3 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:root:PJRT is now the default runtime. For more information, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md\n",
      "WARNING:root:Defaulting to PJRT_DEVICE=CPU\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1712080012.005774   19817 cpu_client.cc:370] TfrtCpuClient created.\n",
      "Using auto half precision backend\n",
      "Currently training with a batch size of: 32\n",
      "***** Running training *****\n",
      "  Num examples = 10,295\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Training with DataParallel so batch size has been adjusted to: 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3,220\n",
      "  Number of trainable parameters = 787,968\n",
      "/tmp/ipykernel_19817/2909531962.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "The following columns in the training set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3220' max='3220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3220/3220 13:59, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.282369</td>\n",
       "      <td>0.885781</td>\n",
       "      <td>0.563208</td>\n",
       "      <td>0.263756</td>\n",
       "      <td>0.839047</td>\n",
       "      <td>0.551296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.216501</td>\n",
       "      <td>0.918026</td>\n",
       "      <td>0.750590</td>\n",
       "      <td>0.555125</td>\n",
       "      <td>0.895393</td>\n",
       "      <td>0.694847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.183907</td>\n",
       "      <td>0.929293</td>\n",
       "      <td>0.800554</td>\n",
       "      <td>0.630609</td>\n",
       "      <td>0.900269</td>\n",
       "      <td>0.748375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.167434</td>\n",
       "      <td>0.935509</td>\n",
       "      <td>0.825423</td>\n",
       "      <td>0.669846</td>\n",
       "      <td>0.902291</td>\n",
       "      <td>0.778836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.157690</td>\n",
       "      <td>0.939005</td>\n",
       "      <td>0.834612</td>\n",
       "      <td>0.689060</td>\n",
       "      <td>0.914727</td>\n",
       "      <td>0.786215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.151123</td>\n",
       "      <td>0.940559</td>\n",
       "      <td>0.840426</td>\n",
       "      <td>0.698386</td>\n",
       "      <td>0.914988</td>\n",
       "      <td>0.793830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.146625</td>\n",
       "      <td>0.942890</td>\n",
       "      <td>0.848186</td>\n",
       "      <td>0.711839</td>\n",
       "      <td>0.917758</td>\n",
       "      <td>0.803234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.144296</td>\n",
       "      <td>0.944833</td>\n",
       "      <td>0.854535</td>\n",
       "      <td>0.722918</td>\n",
       "      <td>0.920008</td>\n",
       "      <td>0.811071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.142761</td>\n",
       "      <td>0.944833</td>\n",
       "      <td>0.854535</td>\n",
       "      <td>0.722918</td>\n",
       "      <td>0.920008</td>\n",
       "      <td>0.811071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.142338</td>\n",
       "      <td>0.944833</td>\n",
       "      <td>0.854535</td>\n",
       "      <td>0.722918</td>\n",
       "      <td>0.920008</td>\n",
       "      <td>0.811071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2574\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/tmp/ipykernel_19817/2909531962.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2574\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/tmp/ipykernel_19817/2909531962.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2574\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/tmp/ipykernel_19817/2909531962.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2574\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/tmp/ipykernel_19817/2909531962.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2574\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/tmp/ipykernel_19817/2909531962.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2574\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/tmp/ipykernel_19817/2909531962.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2574\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/tmp/ipykernel_19817/2909531962.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2574\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19817/2909531962.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2574\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/tmp/ipykernel_19817/2909531962.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "/home/mourad/miniconda3/envs/mistral_dna/lib/python3.8/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2574\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3220, training_loss=0.16700507691187888, metrics={'train_runtime': 840.2538, 'train_samples_per_second': 122.523, 'train_steps_per_second': 3.832, 'total_flos': 2.3131569510144e+16, 'train_loss': 0.16700507691187888, 'epoch': 10.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### CREATE AND TRAIN MODEL\n",
    "num_labels=2\n",
    "\n",
    "model=transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    output_hidden_states=False,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda:0',\n",
    ")\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model = accelerator.prepare_model(model)\n",
    "\n",
    "# Setup trainer\n",
    "trainer = transformers.Trainer(model=model,\n",
    "                               args=training_args,\n",
    "                               compute_metrics=compute_metrics,\n",
    "                               train_dataset=train_dataset, \n",
    "                               eval_dataset=val_dataset,\n",
    "                               data_collator=data_collator,\n",
    "                              callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "                              )        \n",
    "trainer.local_rank=training_args.local_rank\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19817/2909531962.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100% |#######################################################################################|\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9341054117511488"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PREDICT ON TEST DATA\n",
    "bst=32\n",
    "pred_test=[]\n",
    "idx=range(0,len(test_dataset),bst)\n",
    "pbar = ProgressBar()\n",
    "for i in pbar(idx):\n",
    "    batch=test_dataset[i:(i+bst)]\n",
    "    output = model(batch['input_ids'].cuda())\n",
    "    pred_test.append(output[0][:,1].detach())\n",
    "    del output, batch\n",
    "    gc.collect()\n",
    "\n",
    "y_pred=torch.concatenate(pred_test,dim=0)\n",
    "y_pred=torch.sigmoid(y_pred)\n",
    "y_pred=y_pred.cpu().detach().numpy()\n",
    "\n",
    "roc_auc_score(test_dataset.labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral_dna",
   "language": "python",
   "name": "mistral_dna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
