{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Raphael Mourad\n",
    "### Associate Professor\n",
    "### University Paul Sabatier / INRAE MIAT Lab Toulouse\n",
    "### 13/02/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoModelForPreTraining\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import DataCollatorForLanguageModeling, TextDataset\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark=True\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32 \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET DIRECTORY\n",
    "os.chdir(\"/home/mourad/Téléchargements/Mistral-chem/\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN YOUR OWN TOKENIZER\n",
    "vocab_size=1024\n",
    "file_tokenizer=\"data/tokenizer/mistral-chem-\"+str(vocab_size)\n",
    "\n",
    "if os.path.isfile(file_tokenizer)==False:\n",
    "    from tokenizers import Tokenizer\n",
    "    from tokenizers.models import BPE\n",
    "    from transformers import PreTrainedTokenizerFast\n",
    "    from tokenizers.pre_tokenizers import Whitespace\n",
    "    \n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "    from tokenizers.trainers import BpeTrainer\n",
    "    trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
    "                        vocab_size=vocab_size, min_frequency=2)\n",
    "\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    files = [\"data/chemistry/zinc/250k_rndm_zinc_drugs_clean_sorted.txt\"]\n",
    "    tokenizer.train(files, trainer)\n",
    "\n",
    "    tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_object=tokenizer,\n",
    "        unk_token=\"[UNK]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        cls_token=\"[CLS]\",\n",
    "        sep_token=\"[SEP]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "    )\n",
    "\n",
    "    tokenizer.save_pretrained(file_tokenizer)\n",
    "\n",
    "    tokenizer.tokenize(\"C1CSCCSCCS1\", padding=\"longest\", truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LOAD MIXTRAL MODEL CONFIGURATION\n",
    "config = AutoConfig.from_pretrained(\"data/models/Mixtral-8x7B-v0.1-chem\")\n",
    "model = AutoModelForCausalLM.from_config(config,attn_implementation=\"flash_attention_2\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMBER OF MODEL PARAMETERS\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model size: {pytorch_total_params/1000**2:.1f}M parameters\")\n",
    "print(f\"Model size: {pytorch_total_params:.1f} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD BPE LETTER TOKENIZER\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"data/tokenizer/mistral-chem-1024/\", trust_remote_code=True)\n",
    "tokenizer.pad_token = '[EOS]'\n",
    "tokenizer.padding_side  = 'left'\n",
    "print(tokenizer)\n",
    "\n",
    "encoding = tokenizer(\"C1CSCCSCCS1\", padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LOAD DATA \n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "dataset_text = load_dataset(\"csv\", data_files=\"data/chemistry/zinc/250k_rndm_zinc_drugs_clean_sorted.csv.gz\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "dataset = dataset_text.map(tokenize_function, batched=True)\n",
    "print(dataset[\"train\"])\n",
    "\n",
    "train_size = int(0.8 * len(dataset[\"train\"]))\n",
    "test_size = len(dataset[\"train\"]) - train_size\n",
    "train_set, val_set = torch.utils.data.random_split(dataset[\"train\"], [train_size, test_size])\n",
    "train_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PARAMETERS FOR FINE-TUNING\n",
    "batchsize=64 # 1024 for 200b v0.1\n",
    "training_args = TrainingArguments(\n",
    "        output_dir='./results/models',\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        num_train_epochs=50,\n",
    "        per_device_train_batch_size=batchsize,\n",
    "        per_device_eval_batch_size=batchsize,\n",
    "        learning_rate=1e-3, # 5e-4 for v0.1\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        load_best_model_at_end=True,\n",
    "        fp16=True,\n",
    "        gradient_accumulation_steps=32,# 50 for v0.1\n",
    "        report_to=['tensorboard'],\n",
    ")\n",
    "\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PRETRAIN MODEL\n",
    "# 24h / 50 epochs for an RTX3090\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    " \n",
    "print ('Start a trainer...')\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral_dna",
   "language": "python",
   "name": "mistral_dna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
